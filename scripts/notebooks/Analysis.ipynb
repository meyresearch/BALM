{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe29554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5d58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"/rds/user/co-gora1/hpc-work/DL_BA/scripts/notebooks/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39b51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 128,160 || all params: 148,923,641 || trainable%: 0.08605752527901195\n",
      "trainable params: 221,184 || all params: 3,648,624 || trainable%: 6.062120952994882\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from dl_ba import common_utils\n",
    "from dl_ba.configs import Configs\n",
    "from dl_ba.model import BindingAffinityModel\n",
    "\n",
    "# Choose the config path\n",
    "config_filepath = \"/rds/user/co-gora1/hpc-work/DL_BA/configs/random_seed_experiments/bindingdb_random/esm_lokr_chemberta_loha_cosinemse_1.yaml\"\n",
    "configs = Configs(**common_utils.load_yaml(config_filepath))\n",
    "\n",
    "model = BindingAffinityModel(configs.model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6848cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a47ed45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BindingAffinityModel(\n",
       "  (protein_model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoKrModel(\n",
       "      (model): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 640, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-29): 30 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=640, out_features=640, bias=True\n",
       "                    (lokr_w1): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 20x20])\n",
       "                    (lokr_w1_a): ParameterDict()\n",
       "                    (lokr_w1_b): ParameterDict()\n",
       "                    (lokr_w2): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 32x32])\n",
       "                    (lokr_w2_a): ParameterDict()\n",
       "                    (lokr_w2_b): ParameterDict()\n",
       "                    (lokr_t2): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=640, out_features=640, bias=True\n",
       "                    (lokr_w1): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 20x20])\n",
       "                    (lokr_w1_a): ParameterDict()\n",
       "                    (lokr_w1_b): ParameterDict()\n",
       "                    (lokr_w2): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 32x32])\n",
       "                    (lokr_w2_a): ParameterDict()\n",
       "                    (lokr_w2_b): ParameterDict()\n",
       "                    (lokr_t2): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=640, out_features=640, bias=True\n",
       "                    (lokr_w1): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 20x20])\n",
       "                    (lokr_w1_a): ParameterDict()\n",
       "                    (lokr_w1_b): ParameterDict()\n",
       "                    (lokr_w2): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 32x32])\n",
       "                    (lokr_w2_a): ParameterDict()\n",
       "                    (lokr_w2_b): ParameterDict()\n",
       "                    (lokr_t2): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=640, out_features=2560, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pooler): EsmPooler(\n",
       "          (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drug_model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoHaModel(\n",
       "      (model): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "          (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 384)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.144, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-2): 3 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=384, out_features=384, bias=True\n",
       "                    (hada_w1_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w1_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_w2_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w2_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_t1): ParameterDict()\n",
       "                    (hada_t2): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=384, out_features=384, bias=True\n",
       "                    (hada_w1_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w1_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_w2_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w2_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_t1): ParameterDict()\n",
       "                    (hada_t2): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=384, out_features=384, bias=True\n",
       "                    (hada_w1_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w1_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_w2_a): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 384x16])\n",
       "                    (hada_w2_b): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x384])\n",
       "                    (hada_t1): ParameterDict()\n",
       "                    (hada_t2): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.109, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.144, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.144, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): RobertaPooler(\n",
       "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (protein_projection): Linear(in_features=640, out_features=512, bias=True)\n",
       "  (drug_projection): Linear(in_features=384, out_features=512, bias=True)\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ed9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PEFT and base model\n",
    "\n",
    "protein_model = model.protein_model\n",
    "merged_protein_model = protein_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b15663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPLRHWGMARGSKPVGDGAQPMAAMGGLKVLLHWAGPGGGEPWVTFSESSLTAEEVCIHIAHKVGITPPCFNLFALFDAQAQVWLPPNHILEIPRDASLMLYFRIRFYFRNWHGMNPREPAVYRCGPPGTEASSDQTAQGMQLLDPASFEYLFEQGKHEFVNDVASLWELSTEEEIHHFKNESLGMAFLHLCHLALRHGIPLEEVAKKTSFKDCIPRSFRRHIRQHSALTRLRLRNVFRRFLRDFQPGRLSQQMVMVKYLATLERLAPRFGTERVPVCHLRLLAQAEGEPCYIRDSGVAPTDPGPESAAGPPTHEVLVTGTGGIQWWPVEEEVNKEEGSSGSSGRNPQASLFGKKAKAHKAVGQPADRPREPLWAYFCDFRDITHVVLKEHCVSIHRQDNKCLELSLPSRAAALSFVSLVDGYFRLTADSSHYLCHEVAPPRLVMSIRDGIHGPLLEPFVQAKLRPEDGLYLIHWSTSHPYRLILTVAQRSQAPDGMQSLRLRKFPIEQQDGAFVLEGWGRSFPSVRELGAALQGCLLRAGDDCFSLRRCCLPQPGETSNLIIMRGARASPRTLNLSQLSFHRVDQKEITQLSHLGQGTRTNVYEGRLRVEGSGDPEEGKMDDEDPLVPGRDRGQELRVVLKVLDPSHHDIALAFYETASLMSQVSHTHLAFVHGVCVRGPENIMVTEYVEHGPLDVWLRRERGHVPMAWKMVVAQQLASALSYLENKNLVHGNVCGRNILLARLGLAEGTSPFIKLSDPGVGLGALSREERVERIPWLAPECLPGGANSLSTAMDKWGFGATLLEICFDGEAPLQSRSPSEKEHFYQRQHRLPEPSCPQLATLTSQCLTYEPTQRPSFRTILRDLTRLQPHNLADVLTVNPDSPASDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADCGPQHRSGWKQEIDILRTLYHEHIIKYKGCCEDQGEKSLQLVMEYVPLGSLRDYLPRHSIGLAQLLLFAQQICEGMAYLHAQHYIHRDLAARNVLLDNDRLVKIGDFGLAKAVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDSSQSPPTKFLELIGIAQGQMTVLRLTELLERGERLPRPDKCPCEVYHLMKNCWETEASFRPTFENLIPILKTVHEKYQGQAPSVFSVC\n"
     ]
    }
   ],
   "source": [
    "# Get sequence to encode (TYK2)\n",
    "\n",
    "from tdc.multi_pred import DTI\n",
    "data = DTI(name = 'DAVIS')\n",
    "split = data.get_split()\n",
    "\n",
    "target_sequence_ids = data.entity2_idx.loc[data.entity2_idx == \"TYK2(JH2domain-pseudokinase)\"]\n",
    "target_sequence_id = target_sequence_ids.keys()[0]\n",
    "\n",
    "target_sequence = data.entity2.iloc[target_sequence_id]\n",
    "print(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c14303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "protein_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    configs.model_configs.protein_model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07338400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertviz in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers>=2.0 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (4.32.1)\n",
      "Requirement already satisfied: torch>=1.0 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (4.66.1)\n",
      "Requirement already satisfied: boto3 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (1.34.25)\n",
      "Requirement already satisfied: requests in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (2.31.0)\n",
      "Requirement already satisfied: regex in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (2023.10.3)\n",
      "Requirement already satisfied: sentencepiece in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from bertviz) (0.1.99)\n",
      "Requirement already satisfied: filelock in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (4.7.1)\n",
      "Requirement already satisfied: sympy in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (1.11.1)\n",
      "Requirement already satisfied: networkx in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from torch>=1.0->bertviz) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages/huggingface_hub-0.19.0-py3.8.egg (from transformers>=2.0->bertviz) (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.4.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.25 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from boto3->bertviz) (1.34.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from boto3->bertviz) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from requests->bertviz) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from requests->bertviz) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from requests->bertviz) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.25->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /rds/user/co-gora1/hpc-work/miniconda3/envs/dl_ba/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.25->boto3->bertviz) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d8b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 30\n",
      "Size: torch.Size([1, 20, 1189, 1189])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-8b9fd5cc6a4a48399b1539c83957607e\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertviz import head_view, model_view\n",
    "\n",
    "inputs = protein_tokenizer(target_sequence, return_tensors='pt')\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "# token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "with torch.no_grad():\n",
    "    attentions = merged_protein_model(input_ids, output_attentions=True)[\"attentions\"]\n",
    "    print(f\"Layers: {len(attentions)}\")\n",
    "    print(f\"Size: {attentions[0].size()}\")\n",
    "\n",
    "tokens = protein_tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "# To heavy for me macbook\n",
    "# model_view(attentions, tokens)\n",
    "\n",
    "head_view(attentions, tokens, include_layers=[0, 15, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cda6f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-25c390fd6b79413cb438a6c670c268b6\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attentions, tokens, include_layers=[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
